{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Explainability \"mean decrease importance\" and feature permutations.\n",
        "There are multiple ways to measure feature importance. Mean decrease importance is one of the simplest and, in spite of the fact of its limitations, it can be used in a very fast way."
      ],
      "metadata": {
        "id": "iaJBL_xbqiLG",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 1\n",
        "\n",
        "Our first example will use a model that predicts whether a soccer/football team will have the \"Man of the Game\" winner based on the team's statistics. The \"Man of the Game\" award is given to the best player in the game. \n",
        "\nLet's start by reading a dataset and training a black box classifier."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn import metrics\n",
        "\n",
        "blackboxmethod = RandomForestClassifier\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/DataScienceUB/ExplainableDataScience/master/FIFA%202018%20Statistics.csv')\n",
        "\n",
        "y = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\n",
        "feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\n",
        "X = data[feature_names]\n",
        "\nX"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
        "blackboxmodel = blackboxmethod(random_state=0).fit(train_X, train_y)\n",
        "\n",
        "blackboxmodel.fit(val_X, val_y)\n",
        "y_pred=blackboxmodel.predict(val_X)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(val_y, y_pred))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ELI5** is a Python package which helps to debug machine learning classifiers and explain their predictions. \n",
        "\n",
        "**ELI5** provides a way to compute feature importances for any black-box estimator by measuring how score decreases  when a feature is not available; the method is also known as “Mean Decrease Accuracy (MDA)”.\n",
        "\n",
        "To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. \n",
        "\n",
        "To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
        "\n",
        "The method is most suitable for computing feature importances when a number of columns (features) is not huge; it can be resource-intensive otherwise.\n",
        "\nThe next code line install the package."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install eli5"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": true,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can call the method."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm = PermutationImportance(blackboxmodel, random_state=1).fit(val_X, val_y)\n",
        "eli5.show_weights(perm, feature_names = val_X.columns.tolist())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values towards the top are the most important features, and those towards the bottom matter least.\n",
        "\n",
        "The first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric).\n",
        "\n",
        "Like most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next.\n",
        "\nYou'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. "
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: Check if another model, such as a SVM, gets the same scores. All you have to do is to change the `blackboxmethod` to be a SVM.\n",
        "\n",
        "The only change you need to do is to change your black box: \n",
        "\n",
        "`from sklearn.svm import SVC`\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: What is your interpretation of the differences?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2\n",
        "\n",
        "Now we are going to work with a sample of data from the [Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction) competition.\n",
        "\nThe task is to predict the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. We will use two methods: linear regression and Random Forest Regression."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/DataScienceUB/ExplainableDataScience/master/taxi.csv', nrows=50000)\n",
        "\n",
        "# Remove data with extreme outlier coordinates or negative fares\n",
        "data = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +\n",
        "                  'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +\n",
        "                  'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +\n",
        "                  'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +\n",
        "                  'fare_amount > 0'\n",
        "                  )\n",
        "\n",
        "y = data.fare_amount\n",
        "\n",
        "base_features = ['pickup_longitude',\n",
        "                 'pickup_latitude',\n",
        "                 'dropoff_longitude',\n",
        "                 'dropoff_latitude',\n",
        "                 'passenger_count']\n",
        "\n",
        "X = data[base_features]\n",
        "\n\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
        "first_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)\n",
        "\n",
        "first_model.fit(val_X, val_y)\n",
        "y_pred=first_model.predict(val_X)\n",
        "print('Variance score: %.2f' % r2_score(val_y, y_pred))\n",
        "print(\"Mean squared error: %.2f\"\n",
        "      % mean_squared_error(val_y, y_pred))\n",
        "\n",
        "perm = PermutationImportance(first_model, random_state=1).fit(val_X, val_y)\n",
        "eli5.show_weights(perm, feature_names = val_X.columns.tolist())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=2)\n",
        "first_model = LinearRegression().fit(train_X, train_y)\n",
        "\n",
        "first_model.fit(val_X, val_y)\n",
        "y_pred=first_model.predict(val_X)\n",
        "print('Variance score: %.2f' % r2_score(val_y, y_pred))\n",
        "print(\"Mean squared error: %.2f\"\n",
        "      % mean_squared_error(val_y, y_pred))\n",
        "\n",
        "perm = PermutationImportance(first_model, random_state=1).fit(val_X, val_y)\n",
        "eli5.show_weights(perm, feature_names = val_X.columns.tolist())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Question: ** Could you explain in a clear way the result of this experiment?"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "ExplainableMachineLearningShap.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}